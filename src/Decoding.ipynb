{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e83985",
   "metadata": {},
   "source": [
    "# Brain Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca9f10",
   "metadata": {},
   "source": [
    "## Importing  basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9a8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "import pandas as pd\n",
    "from ridge import bootstrap_ridge\n",
    "from evalute import *\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from Feature_extraction import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe28e8",
   "metadata": {},
   "source": [
    "## Loading Fmri Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6dc4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ses2clip_train = {'007': ['01','02','03'],\n",
    "            '008': ['05','06','07','08','09']\n",
    "           }\n",
    "\n",
    "ses2clip_test = {'009': ['04','10','11','12']}\n",
    "\n",
    "fmri_original_train = []\n",
    "fmri_original_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5305ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ses in list(ses2clip_train.keys()):\n",
    "    for clip in ses2clip_train[ses]:\n",
    "        fmri_filepath = '../../../Desktop/movie10.fmriprep/sub-01/ses-'+ses+'/func/sub-01_ses-'+ses+'_task-figures'+clip+'_run-1_space-fsLR_den-91k_bold.dtseries.nii'\n",
    "        img = nib.load(fmri_filepath)\n",
    "        fmri_original_train.append(img.get_data())\n",
    "        \n",
    "for ses in list(ses2clip_test.keys()):\n",
    "    for clip in ses2clip_test[ses]:\n",
    "        fmri_filepath = '../../../Desktop/movie10.fmriprep/sub-01/ses-'+ses+'/func/sub-01_ses-'+ses+'_task-figures'+clip+'_run-1_space-fsLR_den-91k_bold.dtseries.nii'\n",
    "        img = nib.load(fmri_filepath)\n",
    "        fmri_original_test.append(img.get_data())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6940b031",
   "metadata": {},
   "source": [
    "## Loading Stimuli and extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff7f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_feats_train = []\n",
    "audio_feats_train = []\n",
    "\n",
    "video_feats_test = []\n",
    "audio_feats_test = []\n",
    "\n",
    "## Extracting audio and video features from stimuli\n",
    "\n",
    "# for ses in list(ses2clip_train.keys()):\n",
    "#     for clip in ses2clip_train[ses]:\n",
    "#         clip_filepath = '../../../Desktop/movie10.stimuli/figures/figures'+clip+'.mkv'\n",
    "#         tr_time = json.load(open('../../../Desktop/movie10.fmriprep/sourcedata/movie10/task-figures'+clip+'_bold.json'))['RepetitionTime']\n",
    "#         clip_video_feats, clip_audio_feats = extract_stimuli_features(clip_filepath,tr_time)\n",
    "        \n",
    "#         video_feats_train.append(clip_video_feats)\n",
    "#         audio_feats_train.append(clip_audio_feats)\n",
    "        \n",
    "# for ses in list(ses2clip_test.keys()):\n",
    "#     for clip in ses2clip_test[ses]:\n",
    "#         clip_filepath = '../../../Desktop/movie10.stimuli/figures/figures'+clip+'.mkv'\n",
    "#         tr_time = json.load(open('../../../Desktop/movie10.fmriprep/sourcedata/movie10/task-figures'+clip+'_bold.json'))['RepetitionTime']\n",
    "#         clip_video_feats, clip_audio_feats = extract_stimuli_features(clip_filepath,tr_time)\n",
    "        \n",
    "#         video_feats_test.append(clip_video_feats)\n",
    "#         audio_feats_test.append(clip_audio_feats)\n",
    "\n",
    "\n",
    "\n",
    "## Loading pre-extracted features\n",
    "for ses in list(ses2clip_train.keys()):\n",
    "    for clip in ses2clip_train[ses]:\n",
    "        video_feats_filepath = '../data/figures'+clip+'_video.npy'\n",
    "        audio_feats_filepath = '../data/figures'+clip+'_audio.npy'\n",
    "                \n",
    "        video_feats_train.append(np.load(video_feats_filepath))\n",
    "        audio_feats_train.append(np.load(audio_feats_filepath))\n",
    "        \n",
    "for ses in list(ses2clip_test.keys()):\n",
    "    for clip in ses2clip_test[ses]:\n",
    "        video_feats_filepath = '../data/figures'+clip+'_video.npy'\n",
    "        audio_feats_filepath = '../data/figures'+clip+'_audio.npy'\n",
    "                \n",
    "        video_feats_test.append(np.load(video_feats_filepath))\n",
    "        audio_feats_test.append(np.load(audio_feats_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeae4ed",
   "metadata": {},
   "source": [
    "## Creating Input and Target Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd38483",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "\n",
    "final_audio_feats_train = []\n",
    "final_video_feats_train = []\n",
    "final_fmri_train = []\n",
    "\n",
    "final_audio_feats_test = []\n",
    "final_video_feats_test = []\n",
    "final_fmri_test = []\n",
    "\n",
    "for i in range(len(audio_feats_train)):\n",
    "    for j in range(k,audio_feats_train[i].shape[0]):\n",
    "        final_audio_feats_train.append(np.concatenate(audio_feats_train[i][j-k:j]))\n",
    "        final_video_feats_train.append(np.concatenate(video_feats_train[i][j-k:j]))\n",
    "        final_fmri_train.append(fmri_original_train[i][j])\n",
    "        \n",
    "for i in range(len(audio_feats_test)):\n",
    "    for j in range(k,audio_feats_test[i].shape[0]):\n",
    "        final_audio_feats_test.append(np.concatenate(audio_feats_test[i][j-k:j]))\n",
    "        final_video_feats_test.append(np.concatenate(video_feats_test[i][j-k:j]))\n",
    "        final_fmri_testv.append(fmri_original_test[i][j])\n",
    "    \n",
    "final_fmri_train = stats.zscore(np.array(final_fmri_train))\n",
    "final_audio_feats_train = stats.zscore(np.array(final_audio_feats_train))\n",
    "final_video_feats_train = stats.zscore(np.array(final_video_feats_train))\n",
    "final_fmri_test = stats.zscore(np.array(final_fmri_test))\n",
    "final_audio_feats_test = stats.zscore(np.array(final_audio_feats_test))\n",
    "final_video_feats_test = stats.zscore(np.array(final_video_feats_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8c1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fmri_train = np.nan_to_num(final_fmri_train)\n",
    "final_fmri_test = np.nan_to_num(final_fmri_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423e35d",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca88f2",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adcb5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(1, 3, 10) # Equally log-spaced alphas between 10 and 1000. The third number is the number of alphas to test.\n",
    "nboots = 1 # Number of cross-validation runs.\n",
    "chunklen = 40 # \n",
    "nchunks = 20\n",
    "\n",
    "X_train = final_fmri_train\n",
    "X_test = final_fmri_test\n",
    "\n",
    "y_train = final_audio_feats_train\n",
    "y_test = final_audio_feats_test\n",
    "\n",
    "wt, corr, alphas, bscorrs, valinds = bootstrap_ridge(X_train, y_train, X_test, y_test,\n",
    "                                                     alphas, nboots, chunklen, nchunks,\n",
    "                                                     singcutoff=1e-10, single_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bce3be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.matmul(X_test,wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0151aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = y_test\n",
    "predicted = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f6b8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = stats.zscore(predicted)\n",
    "predicted = np.nan_to_num(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb32a4",
   "metadata": {},
   "source": [
    "## Evaluation of Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85dbc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_acc = acc_pairwise(actual, predicted)\n",
    "r_acc = acc_rankBased(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56790d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980379746835443 0.9311708860759493\n"
     ]
    }
   ],
   "source": [
    "print(p_acc, r_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
