{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e83985",
   "metadata": {},
   "source": [
    "# Brain Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422211ba",
   "metadata": {},
   "source": [
    "As discusses earlier, Brain Encoding is the process of learning the mapping d from the stimuli S back from the neural activation F.\n",
    "\n",
    "We shall now try to implement a Ridge Regression-based Brain Decoding model using movie-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca9f10",
   "metadata": {},
   "source": [
    "## Importing  basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9a8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "import pandas as pd\n",
    "from ridge import bootstrap_ridge\n",
    "from evalute import *\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from Feature_extraction import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a32fb",
   "metadata": {},
   "source": [
    "We will be using the \"figures\" movie data from the movie-10 dataset. This figures dataset includes about 120 minutes of functional data for all 6 participants. This movie was presented twice, but we'll go with first run of the movie. We will be working with one subject for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe28e8",
   "metadata": {},
   "source": [
    "## Loading Fmri Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dc4cd",
   "metadata": {},
   "source": [
    "The movie was divided into 12 smaller clips and and they were presented in different sessions. The Fmri was recorded after every 1.49 seconds (tr_time) while subjects viewed the video.\n",
    "\n",
    "We take 2 sessions data in the train set and 1 session data in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6dc4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ses2clip_train = {'007': ['01','02','03'],\n",
    "            '008': ['05','06','07','08','09']\n",
    "           }\n",
    "\n",
    "ses2clip_test = {'009': ['04','10','11','12']}\n",
    "\n",
    "fmri_original_train = []\n",
    "fmri_original_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5305ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ses in list(ses2clip_train.keys()):\n",
    "    for clip in ses2clip_train[ses]:\n",
    "        fmri_filepath = '../../../Desktop/movie10.fmriprep/sub-01/ses-'+ses+'/func/sub-01_ses-'+ses+'_task-figures'+clip+'_run-1_space-fsLR_den-91k_bold.dtseries.nii'\n",
    "        img = nib.load(fmri_filepath)\n",
    "        fmri_original_train.append(img.get_data())\n",
    "        \n",
    "for ses in list(ses2clip_test.keys()):\n",
    "    for clip in ses2clip_test[ses]:\n",
    "        fmri_filepath = '../../../Desktop/movie10.fmriprep/sub-01/ses-'+ses+'/func/sub-01_ses-'+ses+'_task-figures'+clip+'_run-1_space-fsLR_den-91k_bold.dtseries.nii'\n",
    "        img = nib.load(fmri_filepath)\n",
    "        fmri_original_test.append(img.get_data())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6940b031",
   "metadata": {},
   "source": [
    "## Loading Stimuli and extracting Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f57fb5",
   "metadata": {},
   "source": [
    "We will be extracting Visual and Audio features from the the stimuli i.e video clips. \n",
    "Each of the 12 clips were broken down into 1.49 second smaller clips and for each of them, we extract visual and ausio features. We can use librosa library to extract MFCC features for the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff7f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_feats_train = []\n",
    "audio_feats_train = []\n",
    "\n",
    "video_feats_test = []\n",
    "audio_feats_test = []\n",
    "\n",
    "## Extracting audio and video features from stimuli\n",
    "\n",
    "# for ses in list(ses2clip_train.keys()):\n",
    "#     for clip in ses2clip_train[ses]:\n",
    "#         clip_filepath = '../../../Desktop/movie10.stimuli/figures/figures'+clip+'.mkv'\n",
    "#         tr_time = json.load(open('../../../Desktop/movie10.fmriprep/sourcedata/movie10/task-figures'+clip+'_bold.json'))['RepetitionTime']\n",
    "#         clip_video_feats, clip_audio_feats = extract_stimuli_features(clip_filepath,tr_time)\n",
    "        \n",
    "#         video_feats_train.append(clip_video_feats)\n",
    "#         audio_feats_train.append(clip_audio_feats)\n",
    "        \n",
    "# for ses in list(ses2clip_test.keys()):\n",
    "#     for clip in ses2clip_test[ses]:\n",
    "#         clip_filepath = '../../../Desktop/movie10.stimuli/figures/figures'+clip+'.mkv'\n",
    "#         tr_time = json.load(open('../../../Desktop/movie10.fmriprep/sourcedata/movie10/task-figures'+clip+'_bold.json'))['RepetitionTime']\n",
    "#         clip_video_feats, clip_audio_feats = extract_stimuli_features(clip_filepath,tr_time)\n",
    "        \n",
    "#         video_feats_test.append(clip_video_feats)\n",
    "#         audio_feats_test.append(clip_audio_feats)\n",
    "\n",
    "\n",
    "\n",
    "## Loading pre-extracted features\n",
    "for ses in list(ses2clip_train.keys()):\n",
    "    for clip in ses2clip_train[ses]:\n",
    "        video_feats_filepath = '../data/figures'+clip+'_video.npy'\n",
    "        audio_feats_filepath = '../data/figures'+clip+'_audio.npy'\n",
    "                \n",
    "        video_feats_train.append(np.load(video_feats_filepath))\n",
    "        audio_feats_train.append(np.load(audio_feats_filepath))\n",
    "        \n",
    "for ses in list(ses2clip_test.keys()):\n",
    "    for clip in ses2clip_test[ses]:\n",
    "        video_feats_filepath = '../data/figures'+clip+'_video.npy'\n",
    "        audio_feats_filepath = '../data/figures'+clip+'_audio.npy'\n",
    "                \n",
    "        video_feats_test.append(np.load(video_feats_filepath))\n",
    "        audio_feats_test.append(np.load(audio_feats_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeae4ed",
   "metadata": {},
   "source": [
    "## Creating Input and Target Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f1130",
   "metadata": {},
   "source": [
    "There is generally a delay in neural activity peaking and the stimuli onset. Thus to account for this delay, we train our model to predict concat($y_{i-6},y_{i-5},y_{i-4},y_{i-3},y_{i-2},y_{i-1}$) (stimuli features) using $x_{i}$ (Fmri)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd38483",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "\n",
    "final_audio_feats_train = []\n",
    "final_video_feats_train = []\n",
    "final_fmri_train = []\n",
    "\n",
    "final_audio_feats_test = []\n",
    "final_video_feats_test = []\n",
    "final_fmri_test = []\n",
    "\n",
    "for i in range(len(audio_feats_train)):\n",
    "    for j in range(k,audio_feats_train[i].shape[0]):\n",
    "        final_audio_feats_train.append(np.concatenate(audio_feats_train[i][j-k:j]))\n",
    "        final_video_feats_train.append(np.concatenate(video_feats_train[i][j-k:j]))\n",
    "        final_fmri_train.append(fmri_original_train[i][j])\n",
    "        \n",
    "for i in range(len(audio_feats_test)):\n",
    "    for j in range(k,audio_feats_test[i].shape[0]):\n",
    "        final_audio_feats_test.append(np.concatenate(audio_feats_test[i][j-k:j]))\n",
    "        final_video_feats_test.append(np.concatenate(video_feats_test[i][j-k:j]))\n",
    "        final_fmri_testv.append(fmri_original_test[i][j])\n",
    "    \n",
    "final_fmri_train = stats.zscore(np.array(final_fmri_train))\n",
    "final_audio_feats_train = stats.zscore(np.array(final_audio_feats_train))\n",
    "final_video_feats_train = stats.zscore(np.array(final_video_feats_train))\n",
    "final_fmri_test = stats.zscore(np.array(final_fmri_test))\n",
    "final_audio_feats_test = stats.zscore(np.array(final_audio_feats_test))\n",
    "final_video_feats_test = stats.zscore(np.array(final_video_feats_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8c1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fmri_train = np.nan_to_num(final_fmri_train)\n",
    "final_fmri_test = np.nan_to_num(final_fmri_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423e35d",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca88f2",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adcb5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(1, 3, 10) # Equally log-spaced alphas between 10 and 1000. The third number is the number of alphas to test.\n",
    "nboots = 1 # Number of cross-validation runs.\n",
    "chunklen = 40 # \n",
    "nchunks = 20\n",
    "\n",
    "X_train = final_fmri_train\n",
    "X_test = final_fmri_test\n",
    "\n",
    "y_train = final_audio_feats_train\n",
    "y_test = final_audio_feats_test\n",
    "\n",
    "wt, corr, alphas, bscorrs, valinds = bootstrap_ridge(X_train, y_train, X_test, y_test,\n",
    "                                                     alphas, nboots, chunklen, nchunks,\n",
    "                                                     singcutoff=1e-10, single_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bce3be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.matmul(X_test,wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0151aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = y_test\n",
    "predicted = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f6b8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = stats.zscore(predicted)\n",
    "predicted = np.nan_to_num(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb32a4",
   "metadata": {},
   "source": [
    "## Evaluation of Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171d3c7",
   "metadata": {},
   "source": [
    "**Pairwise Accuracy**\n",
    "To measure the pairwise accuracy, the first step is to predict all the test stimulus vector representations using a trained decoder model.\n",
    "\n",
    "Let S = [S$_{0}$, S$_{1}$,$\\cdots$,S$_{n}$] and  $\\hat{S}$ = [$\\hat{S}_{0}$, $\\hat{S}_{1}$,$\\cdots$,$\\hat{S}_{n}$] \n",
    "denote the true and predicted stimulus representations for $n$ test instances resp. \n",
    "\n",
    "Given a pair $(i,j)$ such that $0\\leq i,j\\leq n$, score is 1 if\n",
    "corr(S$_{i}$,$\\hat{S}_{i}$) + corr(S$_{j}$,$\\hat{S}_{j}$) $>$ corr(S$_{i}$,$\\hat{S}_{j}$) + corr(S$_{j}$,$\\hat{S}_{i}$), else 0.\n",
    "\n",
    "Here, corr denotes the Pearson correlation.\n",
    "\n",
    "**Rank Accuracy** \n",
    "\n",
    "We compared each decoded vector to all the true text-derived semantic vectors and ranked them by their correlation. The classification performance reflects the rank $r$ of the text-derived vector for the correct word: $1-\\frac{r-1}{\\#instances-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85dbc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_acc = acc_pairwise(actual, predicted)\n",
    "r_acc = acc_rankBased(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56790d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980379746835443 0.9311708860759493\n"
     ]
    }
   ],
   "source": [
    "print(p_acc, r_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
